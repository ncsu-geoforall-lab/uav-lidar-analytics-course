<section>
    <h2>Imagery processing</h2>
    <h4>GIS/MEA 584:</h4>
    <h4>Mapping and Analysis Using UAS</h4>
    <h4 style="margin-top: 0.5em">
        Justyna Jeziorska</h4>
    <p class="title-foot">
        <a href="https://geospatial.ncsu.edu/" title="Center for Geospatial Analytics">Center for Geospatial Analytics</a>
       <br> <a href="http://www.ncsu.edu/" title="North Carolina State University">North Carolina State University</a>
</section>
<section>
    <h2>Objectives</h2>
    <ul>
        <li class="fragment"><strong>Understand</strong> the photogrammetric data processing as a multistep process;</li>
        <li class="fragment"><strong>Indicate</strong> data needed for orthophoto/DTM generation from aerial imagery;</li>
        <li class="fragment"><strong>Understand</strong> the difference between interior and exterior orientation of the photo;</li>
        <li class="fragment"><strong>Describe</strong> the workflow of geoprocessing of aerial imagery in designated software (Agisoft Photoscan);</li>

    </ul>
</section>

<section>
    <h2>Photogrammetric process</h2>
     <img class="fragment" src="img/photogrammetric_proces.png">
</section>
<section>
    <h2>Photogrammetric process</h2>
     <img class="fragment" src="img/processing/flowchart_processing.png">
<h2 class="fragment">Data processing</h2>
     <img class="fragment" src="img/processing/flowchart_processing_plain.png">
</section> 

<section>
    <h2>UAS data </h2>
    <h4>What do we get after the flight mission?</h4>
        <img class="fragment"src="img/new/uas_data.jpg">

</section>

<section>
    <h2>Digital imagery</h2>
        <img class="fragment"src="img/digital_imagery.png">
 <ul>
        <li class="fragment">usually on the camera SD card</li>
        <li class="fragment">can be geotagged (depends on camera)</li>
        <ul>
        <li class="fragment">Camera lens location is "written into" each photo's EXIF file</li>
	<li class="fragment">this is not necessary the case...</li>
	</ul>
</ul>
</section>
<section>
    <h2>Flight log</h2>
        <ul>
        <li class="fragment">Onboard Inertial Measurement Unit (IMU) accurately measures the orientation of airborne sensors,</li>
        <li class="fragment">Information is logged into a text file (flight log),</li>
        <li class="fragment">Contains elements of exterior orientation (EO, more <a href="./2017_Imagery_Processing.html#/18">later in the lecture</a>)</li>
        </ul>
        <img class="fragment" src="img/log.png" width="80%">
</section>
<section>
    <h2>GCP coordinates</h2>
<div class="left">
        <ul>
        <li class="fragment">Measured by GPS coordinates of the panels set before the flight</li>
        <li class="fragment">Photo ID points (distinguishable ground features)can be surveyed later on </li>
        <li class="fragment">It is important to know the GCPs coorditate system (spatial reference system)</li>
        </ul>
</div>
<div class="right">
        <img src="img/GCPs.png">
</div>
</section>

<section>
    <h2>Spatial reference system</h2>
     <div class="left">
        <ul>
        <li class="small fragment">defines how the two-dimensional, projected map in your GIS is related to real places on the earth</li>
        <li class="small fragment">It is crucial to know what is your data reference system!</li>

    </div>
    <div class="right">
        <img src="img/new/projection.JPG" width="60%">
    </div>
<ul>
        <li class="small fragment">There are global map projections, but most map projections are created and optimized to project smaller areas of the earth’s surface</li>
        <li class="small fragment">There are two different types of coordinate reference systems: Geographic Coordinate Systems and Projected Coordinate Systems</li>
        <li class="small fragment"><a href="http://spatialreference.org/ref/epsg/">Spatial reference list (EPSG codes for coordinate reference systems)</a></li>
</ul>
</section>

<section>
    <h2>UAS data processing outputs</h2>
    <h4>What do we get after processing the data?</h4>
        <img class="fragment" src="img/new/processing_outputs.JPG">
</section>

<section>
    <h2>Orthophoto</h2>
     <div class="left">
        <ul>
        <li class="fragment"> aerial imagery geometrically corrected ("orthorectified") such that the scale is uniform</li>
        <li class="fragment"> raster: consists of red, green and blue bands</li>
    </div>
    <div class="right">
        <img src="img/new/ortho_example.jpg">
    </div>
</section>
<section>
    <h2>Digital Surface Model</h2>
<img src="img/new/dtm_dsm.jpg" width="60%">
        <ul>
        <li class="fragment"> DEM/DTM - Digital Elevation Model / Digital Terrain Model</li>
        	<ul class="fragment">       
		<li> representation of a terrain's elevation</li>
        	<li> bare-earth raster grid</li>
		</ul>
        <li class="fragment"> DSM - Digital Surface Model 
        	<ul class="fragment">       
		<li> representation of a visible surface</li>
        	<li> captures the natural and built features on the Earth’s surface</li>
		</ul>
</section>

<section>
    <h2>Pointcloud</h2>
        <ul>
        <li class="fragment"> representation of the external surface of an object</li>
        <li class="fragment"> set of vertices in a three-dimensional coordinate system</li>
        <li class="fragment"> vector or raster?</li>
        <li class="fragment"> Dale Lutz once said, "point cloud is a badly behaved raster"</li>
 <div class="left">
        <img src="img/new/pointclouds.png">
</div>

   <div class="right">
        <img src="img/new/Point_cloud_torus.gif">
    </div>
</section>


<section>
        <img class="fragment"src="img/new/uas_data.jpg" width="61%">
        <img class="fragment"src="img/processing/getting_there.png" width="40%">
        <img class="fragment" src="img/new/processing_outputs.JPG" width="61%">
</section>
<section>
        <img src="img/new/uas_data.jpg" width="61%">
        <img src="img/processing/getting_there2.png" width="40%">
        <img src="img/new/processing_outputs.JPG" width="61%">
</section>


</section>
<section>
    <h2>Multiple-view geometry</h2>
    <ul>
        <li class="fragment"><strong>Scene geometry (structure):</strong> <br>Given 2D point matches in two or more images, where are the corresponding points in 3D?</li>
        <li class="fragment"><strong>Correspondence (stereo matching):</strong> Given a point in just one image, how does it constrain the position of the corresponding point in another image? </li>
        <li class="fragment"><strong>Camera geometry (motion):</strong> Given a set of corresponding points in two or more images, what are the camera matrices for these views?</li>
    </ul>
</section>

<section>
    <h2>What do we need?</h2>
    <ol>
        <li class="fragment">Digital <strong>imagery</strong>;</li>
        <li class="fragment">(Digital elevation model or topographic dataset)</li>
        <li class="fragment">Exterior <strong>orientation parameters </strong> from aerial triangulation or IMU;</li>
        <li class="fragment">(<strong>Camera calibration</strong> report);</li>
        <li class="fragment">(Ground Control Points parameters);</li>
        <li class="fragment">Photogrammetric<strong> processing software</strong> that utilizes collinearity equations.</li>
    </ol>
<p class="small">items in brackets are optional<p>
</section>
<section>
    <h2>1. Digital imagery</h2>
            <img class="fragment" src="img/digital_imagery.png">
    <div class="left">  
            <img class="fragment" src="img/multiview.png">
    </div>
    <div class="right">
            <img class="fragment" src="img/camera_sensor.png" width="80%">
    </div>
</section>
<section>
    <h2>2. Digital Elevation Model</h2>
     <div class="left">  
		<img src="img/disortion.png">
     </div>
    <div class="right">
           <p class="fragment"> <strong>In the past:</strong> Shape of the ground surface must be known in order to remove the effects of relief displacement<p>
    </div>
             <p class="fragment"> <strong>Now:</strong> computed automatically by Structure from Motion <p>    
</section>

<section>
    <h2>Structure from Motion (SfM)</h2>
    <div class="left">  
        <ul>
        <li class="fragment">range imaging technique,</li>
        <li class="fragment">process of estimating 3D structures from 2D image sequences,</li>
        <li class="fragment">may be coupled with local motion signals</li>
        </ul>
    </div>
    <div class="right">
            <img class="fragment" src="img/sfm_explained.jpg">
    </div>
</section>
<section>
    <h2>3. Exterior orientation (EO) </h2>
            <p class="fragment">EO= <strong>position</strong> and <strong>orientation</strong> in the object space</p>
            <p class="fragment">6 elements <strong>necessary</strong> for any photogrammetric processing:</p>
    <div class="left">  
    <ul>
        <li class="fragment">X, Y, and Z of the exposure station position (latitude, longitude and altitude of the camera),</li>
        <li class="fragment">angular orientation: ω, φ, and κ (yaw, pich and roll)</li>
    </ul>
    </div>
    <div class="right">
            <img class="fragment" src="img/orientation.png">
    </div>
</section>
<section>
    <h3>Flight log</h3>
        <ul>
        <li class="fragment">Log file contains elements of exterior orientation that are measured by onboard Inertial Measurement Unit (IMU) and written into text file</li>
</ul>

        <img src="img/processing/log.png" width=60%>
<ul>
        <li class="fragment">Sometimes (most DJI products) exterior orientation parameters are saved in photos EXIF file</li>
        <li class="fragment">Log contains information about the location of the camera, not location of the depicted object  - <a href="./HM_Photogrammetry_and_SfM.html#/25">more info in this section of lecture 3</a></li>
        </ul>
</section>
<section>
    <h2>4. Interior orientation </h2>
    <div class="left">  
    <ul>
        <li class="fragment">In the past: camera calibration report,</li>
        <li class="fragment">Now: Self-calibration (auto-calibration) is the process of determining intrinsic camera parameters directly from uncalibrated images</li>
    </ul>
    </div>
    <div class="right">
            <img src="img/interior_orientation.png">
    </div>
    <ul>
        <li class="fragment">Can be automatically derived using Structure from Motion (SfM) methods</li>
    </ul>
</section>
<section>
    <h2>5. Ground Control Points</h2>
    <div class="left">
        <li class="fragment"><strong>GCP</strong> - target in the project area with known 3 coordinates (X,Y,Z or lat, long, alt). </li>
        <li class="fragment">For more information about placing targets and importance of  GCPs see <a href="./HM_Photogrammetry_and_SfM.html#/30">this section of lecture 3</a></li>
        <li class="fragment">For more information about processing the data with GCPs see <a href="./2017_Imagery_Processing_assignment_intro.html">intro to the assignment</a></li>
</div>
    <div class="right">
            <img src="img/processing/GCP_girls.png">
    </div>
</section>
<section>
    <h2>Processing options</h2>
<img class="fragment" src="img/new/processing_options.JPG">
</section>

<section>
    <h2>Processing options</h2>
Everything boils down to... money (and time)
<ul>
        <li class="fragment">What is my starting budget and equipment?;</li>
        <li class="fragment">How frequently will I fly?;</li>
        <li class="fragment">Do I have the experience/ training necessary for processing (or am I able to hire people who do)?</li>
        <li class="fragment">Do I have time to process the data by myself?</li>
    </ul>
</section>
<section>
<h2>Processing options - software</h2>

<ul>
        <li class="fragment"><a href="http://www.agisoft.com/">Agisoft Photoscan</a></li>
        <li class="fragment"><a href="https://pix4d.com/">Pix4D</a></li>
        <li class="fragment"><a href="http://uas.trimble.com/tbc-am">Trimble Business Center</a> - Aerial Photogrammetry Module</li>
        <li class="fragment"><a href="http://www.esri.com/products/drone2map">Drone2Map (ESRI)</a></li>
        <li class="fragment"><a href="https://dronemapper.com/">DroneMapper</a></li>
        <li class="fragment"><a href="http://opendronemap.org/">opendronemap</a></li>
<li class="fragment">many many more...</li>
    </ul>
</section>
<section>
    <h2>Agisoft PhotoScan Professional</h2>
<div class="left">  
    <ul>
        <li class="fragment">Image-based solution aimed at creating 3D content from still images;</li>
        <li class="fragment">Operates with arbitrary images and is efficient in both controlled and uncontrolled conditions;</li>
        <li class="fragment">Both image alignment and 3D model reconstruction are fully automated.</li>
    </ul>
    </div>
    <div class="right">
        <p class="small fragment"><a href="http://www.agisoft.com/downloads/installer/"> Installer</a></p>
        <p class="small fragment"><a href="http://www.agisoft.com/pdf/photoscan-pro_1_3_en.pdf"> Manual</a></p>
        <p class="small fragment">also tutorials for</p>
        <p class="small fragment"><a href="http://www.agisoft.com/pdf/PS_1.3%20-Tutorial%20%28BL%29%20-%20Orthophoto,%20DEM%20%28GCPs%29.pdf"> Orthophoto and DSM generation with GCPs</a><br>
<a href="http://www.agisoft.com/pdf/PS_1.1%20-Tutorial%20%28IL%29%20-%20Classification%20and%20DTM.pdf"> Dense Cloud Classification & DTM Generation</a><br>
<a href="http://www.agisoft.com/pdf/PS_1.1_Tutorial%20%28IL%29%20-%20Volume%20measurements.pdf"> Mesh-based Area & Volume Measurements</a> </p>
        <p class="small fragment"><a href="https://www.youtube.com/channel/UCPheXwPeFLnWHo8u4ksSH7w"> Youtube channel</a></p>
        <img src="img/Agisoft_Logo.jpg">
    </div>    


</section>
<section>
    <h2>Processing workflow</h2>
        <img src="img/processing/flowchart_processing_plain.png">
     <h4 class="fragment">Preprocessing stage:</h4>
        <img class="fragment" src="img/processing/flowchart_processing_pre.png">
            <ul>
                <li class="small fragment">loading photos into PhotoScan;</li>
                <li class="small fragment">inspecting loaded images, removing unnecessary images.</li>
            </ul>  
</section>
<section>
    <h3>Processing workflow</h3>
        <h4 >Processing stage:</h4>
        <img src="img/processing/flowchart_processing_exporting.png">

            <ol>
                <li class="fragment">Aligning photos;</li>
                <li class="fragment">Building dense point cloud; <p class=small>(optional: editing dense point cloud)</p></li>
                <li class="fragment">Building mesh (3D polygonal model); <p class=small>(optional: editing mesh)</p></li>
                <li class="fragment">Generating texture;</li>
                <li class="fragment">Building DSM and orthomosaic</li>
            </ol>

</section>
<section>
    <h3>Processing workflow</h3>
        <h4 >Exporting results</h4>
        <img src="img/processing/flowchart_processing_ex.png">
        <img class="fragment"src="img/new/processing_outputs.JPG">
     
</section>
<section>
    <h3>Preprocessing</h3>
        <ul>
        <li class="fragment">Loading photos,</li>
        <li class="fragment">Loading camera positions (flight log)</li>
        <li class="fragment">If the EO is in the photos EXIF file, the parameters will load automatically</li>
        </ul>
        <img class="fragment" src="img/processing/camera_positions.PNG" width = "70%">
</section>
<section>
    <h3>1. Aligning photos</h3>
            <p class="fragment">At this stage Agisoft PhotoScan: <br >implements SfM algorithms to monitor the movement of features through sequence of multiple images:</p>
    <div class="left">  
    <ul>
        <li class="fragment">obtains the relative location of the acquisition positions,</li>
        <li class="fragment">refines camera calibration parameters, </li>
        <li class="fragment"><strong>sparse point cloud</strong> and a set of <strong>camera positions</strong> are formed.</li>
        </ul>
    </div>
    <div class="right">
            <img class="fragment" src="img/aligning_photos_funny.png" width="60%">
    </div>
</section>
<section>
    <h2>Bundle Block Adjustment</h2>
    <div class="left">  
    <ul>
        <li class="fragment">Non-linear method for refining structure and motion</li>
        <li class="fragment">Minimizing reprojection error</li>
       <li class="fragment">Detecting image feature points (i.e. Various geometrical similarities such as object edges or other speciﬁc details);</li>
    </ul>
    </div>
    <div class="right">
            <img src="img/new/bundle_block_adjustment.JPG">
    </div>
</section>
<section>
    <h3>Bundle Block Adjustment</h3>
    <ul>

        <li class="fragment">Subsequently monitoring the movement of those points throughout the sequence of multiple images; </li>
    </ul>
    <div class="left">  
    <ul>
        <li class="fragment">Using this information as input, the locations of those feature points can be estimated and rendered as a sparse 3D point cloud    </div>
    <div class="right">
            <img src="img/Bundle_Block_Adjustment2.png">
    </div>
</section>
<section>
    <h3>Aligning cameras in PhotoScan</h3>
            <img class="left fragment" src="img/processing/aligned_photos.PNG" width="58.5%">
            <img class="right fragment" src="img/processing/sparse_cloud.PNG" width="39%">
        <p class="small right fragment"><strong>sparse point cloud</strong> generation</p>
        <p class="fragment"><strong>Accuracy</strong></p>
           
<ul>
                <li class="fragment"><strong>High</strong> accuracy setting > more accurate camera position estimates (time consuming);</li>
                <li class="fragment"><strong>Low</strong> accuracy setting > rough camera positions.</li>
            </ul>


</section>
<section>
    <h3>2. Building dense point cloud</h3>
  <p class="fragment">At the stage of dense point cloud generation reconstruction PhotoScan calculates depth maps for every image</p>
 
       <ul>
                <li class="fragment">Quality: <strong>Highest, High, Medium, Low, Lower</strong>>  the higher quality the more accurate camera position estimates but the process is more time consuming</li>
       </ul> 

           <img class="fragment" src= "img/processing/sparse_cloud.PNG" width="40%">
           <img class="fragment" src= "img/processing/arrows.png" width="10%">
           <img class="fragment" src= "img/processing/dense_cloud.PNG" width="40%">

</section>
<section>
    <h3>2. Building dense point cloud</h3>
  
        <p class="fragment"><strong>Depth Filtering modes</strong></p>
 	<p class="fragment">Algorithms sorting outliers <br>(due to some factors, like poor texture of some elements of the scene, noisy or badly focused images)</p> 

       <ul>
                <li class="fragment"><strong>Mild </strong>depth filtering mode > for <strong>complex geometry </strong>(numerous small details on the foreground), for important features not to be sorted out;</li>
                <li class="fragment"><strong>Aggressive </strong>depth filtering mode > sorting out most of the outliers;</li>
                <li class="fragment"><strong>Moderate </strong>depth filtering mode > results in between the Mild and Aggressive</li>
      </ul>   
</section>
<section>
    <h3>Optional: editing dense point cloud</h3>
    <div class="left">
    <ul>


        <li class="fragment">Manual points removal</li>
        <li class="fragment">Automatic filtering based on applied masks;</li>

        <li class="fragment">Sparse cloud only:</li>
            <ul>
        <li class="fragment">Reducing number of points in cloud by setting tie point per photo limit</li>
	<li class="fragment">Automatic filtering based on:</li>
            <ul>
                <li class="small fragment">Reprojection error;</li>
                <li class="small fragment">Reconstruction uncertainty;</li>
                <li class="small fragment">Image count.</li>
            </ul>
    </ul>
    </div>
    <div class="right">
           <img src= "img/processing/editing_pointcloud.PNG">
    </div>
</section>
<section>
    <h3>3. Building mesh</h3>

    <div class="left">     
    <ul>
        <li class="fragment"><strong>Arbitrary</strong> > for modeling of any kind of object</li>
            <ul>
                <li class="small fragment">should be selected for closed objects (statues, buildings, etc.);</li>
                <li class="small fragment">memory consumption: high </li>
            </ul>
</div>
    <div class="right">
            <img class="fragment" src="img/processing/mesh.jpg" width="65%">
    </div>
    <div class="left">     
	<li class="fragment"><strong>High field</strong> > for modeling of planar surfaces</li>
            <ul>
               	<li class="small fragment">should be selected for aerial photography;</li>
                <li class="small fragment">memory consumption: low</li>
                <li class="small fragment">allows for larger data sets processing.</li>
            </ul>
    </ul>
    </div>
 <!--   <div class="right">
            <img class="fragment" src="img/processing/mesh_comparison.PNG" width="120%">
    </div>
-->
</section>
<section>
    <h3>3. Building mesh</h3>
   <div class="right">
            <img src="img/processing/mesh_low.PNG">
            <img src="img/processing/mesh_high.PNG">
    </div>
    <div class="left">     
    <ul>
        <li class="fragment"><strong>Source data</strong> > the source for the mesh generation</li>
            <ul>
                <li class="small fragment"><strong>Sparse cloud</strong> > fast 3D model generation (low quailty)</li>
                <li class="small fragment"><strong>Dense cloud</strong> > high quality output based on the previously reconstructed dense point cloud.</li>
            </ul>
 	<li class="fragment"><strong>Face count </strong> > the maximum face count in the final mesh.</li>
     </ul> 
	</div>
</section>
<section>
    <h3>Optional: editing mesh</h3>
    <div class="left">     
    <ul>
               <li class="fragment"><strong>Close Holes tool</strong> > repairs your model if the reconstruction procedure resulted in a mesh with several holes, due to <strong>insufficient image overlap</strong>necessary step for <strong>volumes calculation</strong>;</li>

    </ul>
    </div>
    <div class="right">
            <img class="fragment" src="img/processing/close_holes.png">
    </div>
            <ul>
         <li class="fragment"><strong>Decimation tool</strong> > decreases the geometric resolution of the model by replacing high resolution mesh with a lower resolution one; </li>
            </ul>    
</section>
<section>
    <h3>Optional: editing mesh</h3>

 <div class="right">
            <img src="img/processing/mesh_editing.PNG" width="80%">
            <img src="img/processing/mesh_editing2.PNG" width="80%">
 </div>
 <div class="left">  
    <ul>
        <li class="fragment"><strong>Automatic filtering</strong> based on specified criterion:</li>
            <ul>
                <li class="fragment">Connected component size,</li>
                <li class="fragment">Polygon size.</li>
            </ul>
        <li class="fragment">Manual polygon removal,</li>
        <li class="fragment">Fixing mesh topology,</li>
        <li class="fragment"><strong>Editing mesh in the external program</strong> <br> export mesh for editing in the external program > import edited mesh</li>
    
    </ul>
    </div>   
</section>
<section>
    <h3>4. Generating texture</h3>
   
    <ul>
    <li class="fragment">Determines how the object texture will be packed in the texture atlas;</li>
    <li class="fragment">Effects the quality of the final model;</li>
</ul>
 <div class="right">
            <img class="fragment" src="img/processing/texture.PNG" width="80%">
 </div>
 <div class="left">  
<ul>
    <li class="fragment"><strong>Texture mapping modes:</strong></li>
            <ul class="fragment">
                <li>Generic,</li>
                <li>Adaptive orthophoto,</li>
                <li>Orthophoto,</li>
                <li>Spherical,</li>
                <li>Single photo,</li>
                <li>Keep uv.</li>
            </ul>
    </ul> 
 </div>   
</section>
<section>
    <h3>4. Generating texture</h3>
   
    <ul>
    <li>Determines how the object texture will be packed in the texture atlas;</li>
    <li>Effects the quality of the final model;</li>
</ul>
 <div class="right">
            <img src="img/processing/texture.PNG" width="80%">
 </div>
 <div class="left">  
<ul>
    <li><strong>Texture mapping modes:</strong></li>
            <ul>
                <li><strong>Generic,</li>
                <li>Adaptive orthophoto,</li>
                <li>Orthophoto,</strong></li>
                <li>Spherical,</li>
                <li>Single photo,</li>
                <li>Keep uv.</li>
            </ul>
    </ul> 
 </div>   
</section>
<section>
    <h3>Texture mapping modes</h3>
<ul>	   
<li class="fragment"><strong>Generic</strong></li> 
	<ul>
	<li class="fragment">creates as uniform texture as possible.</li>
	</ul> 
<li class="fragment"><strong>Adaptive orthophoto</strong></li>
	<ul>
        	<ul>
        	<li class="fragment">The object surface split into the flat part and vertical regions;</li>
        	<li class="fragment">The flat part of the surface textured using the orthographic projection, 
			while vertical regions textured separately to maintain accurate texture representation in such regions;</li>
        	<li class="fragment">More compact texture representation for nearly planar scenes + good texture quality 
			for vertical surfaces.</li>
        	</ul> 
	</ul>
</section>
<section>
    <h3>Texture mapping modes</h3>
	<ul>
     	<li class="fragment"><strong>Orthophoto</strong></li> 
        	<ul>
        	<li class="fragment">The whole object surface textured in the orthographic projection;</li>
        	<li class="fragment">Even more compact texture representation than the Adaptive orthophoto at
				 the expense of texture quality in vertical regions.</li>
      		</ul> 
     	<li class="small fragment">Spherical - Only for objects that have a ball-like form.</li> 
        <li class="small fragment">Single photo - Texture from a single photo (photo can be selected from 'Texture from' list)</li>
        <li class="small fragment">Keep uv: Generates texture atlas using current texture parametrization; Rebuilding current texture with different resolution or generating
			 the atlas parametrized in the external software.</li> 
	</ul>
</section>
<!--
<section>
    <h3>Texture generation parameters</h3>
 <p class="fragment"><strong>Blending mode</strong> (not used in Single photo mode)- selects the way how pixel values will 
		be combined to the final texture</p>
    
    <ul>
      <li class="fragment"><strong>Average</strong> -uses the average value of all pixels from individual photos</li>
      <li class="fragment"><strong>Mosaic</strong> gives more quality for orthophoto and texture atlas than <strong>Average mode</strong>, since it <strong>does not mix image details of overlapping photos, but uses most appropriate</strong></li>
      <li class="fragment"><strong>Max Intensity</strong> - selects photo which has maximum intensity of the corresponding pixel</li>
	<li class="fragment"><strong>Min Intensity </strong> - the photo which has minimum intensity of the corresponding pixel is selected</p></li> 
</ul>
</section>
<section>
    <h3>Texture generation parameters</h3>
       <ul>
	<p class="fragment"><strong>Texture size / count</strong></p> 
	<p class="fragment">Specifies the size (width & hight) of the texture atlas in pixels and determines the number of files for texture to be exported to:</li>
            <ul>
                <li class="fragment">several files > archive greater resolution,</li>
                <li class="fragment">single file can fail due to RAM limitations. </li>
              </ul>
        
       <p class="fragment"><strong>Enable color correction</strong></p> 
       		<ul>
           	 <li class="fragment">for processing of data sets with extreme brightness variation,</li>
           	 <li class="fragment">takes up a long time.</li>
        	</ul>
	</ul> 
</section>
-->
<section>
    <h2>Generating DSM</h2>
 <div class="right">
            <img src="img/processing/build_DEM.png">
 </div>
 <div class="left">   
<p> Parameters</p> 
    <ul>
    <li class="fragment"><strong>Source data:</strong> dense point cloud</li>
    <li class="fragment"><strong>Interpolation</strong> </li>
<ul>
    <li class="small fragment">Disabled: leads  to  accurate  reconstruction  results  since  only  areas
corresponding to dense point cloud points are reconstructed</li>
    </ul> 
 </div>
<ul>
<ul>
    <li class="small fragment">Enabled (recommended):  interpolation mode 
PhotoScan will calculate DEM for all areas of thescene  that  are  visible  on  at  least  one  image.  </li>
</ul>
</ul>
</section>
<section>
    <h2>Generating orthophoto</h2>
 <div class="right">

            <img src="img/processing/build_ortho.png" width="70%">
 </div>
 <div class="left">   
<p> Parameters</p> 
    <ul>
    <li class="fragment"><strong>Surface:</strong> DEM</li>
    <li class="fragment"><strong>Bleding mode</strong></li>
<ul>
    <li class="small fragment">Mosaic  (default): implements  approach  with  data  division  into  several  frequency  domains which are blended independently.</li>
    </ul> 
 </div>
<ul>
<ul>
    <li class="small fragment">Average: uses the weighted average value of all pixels from individual photos.</li>
    <li class="small fragment">Disabled: the color value for the pixel is taken from the photo with the camera view being almost
along the normal to the reconstructed surface in that point.</li>
</ul>
</ul>
</section>

<section>
    <h2>Exporting results</h2>
    <h2 class="small"> and saving intermediate results</h2>
 <div class="left">     
    <ul>
    <li class="fragment"><strong>Orthophoto export</strong></li>
    </ul> 
 </div>   
 <div class="right">
            <img class="fragment" src="img/new/ortho_example.jpg">
 </div>
</section>

<section>
    <h2>Exporting results</h2>
    <h2 class="small"> and saving intermediate results</h2>

 <div class="left">     
    <ul>
    <li>Orthophoto export</li>
    <li><strong>DEM export</strong></li>
    </ul> 
 </div> 
 <div class="right">
            <img src="img/processing/DSM.png" width ="150%">
 </div> 
</section>

<section>
    <h2>Exporting results</h2>
    <h2 class="small"> and saving intermediate results</h2>

 <div class="left">     
    <ul>
    <li>Orthophoto export</li>
    <li>DEM export</li>
    <li><strong>3D model export</strong></li>
    </ul> 
 </div> 
 <div class="right">
            <img src="img/new/sketchfab.png" width=72%>
 </div> 
</section>
<section>
    <h2>Exporting results</h2>
    <h2 class="small"> and saving intermediate results</h2>

 <div class="left">     
    <ul>
    <li>Orthophoto export</li>
    <li>DEM export</li>
    <li>3D model export</li>
    <li><strong>Point cloud export</strong></li>
    </ul> 
 </div> 
 <div class="right">
            <img src="img/processing/pointcloud.png">
 </div> 
</section>
<section>
    <h2>Exporting results</h2>
    <h2 class="small"> and saving intermediate results</h2>

 <div class="left">     
    <ul>
    <li>Orthophoto export</li>
    <li>DEM export</li>
    <li>3D model export</li>
    <li>Point cloud export</li>
    <li><strong>Processing report generation</strong></li>
    </ul> 
 </div> 
 <div class="right">
            <img src="img/new/report_1.JPG" width ="60%">
            <img src="img/new/report_2.JPG" width ="60%">
 </div> 
</section>

<section>
    <h2>Processing report</h2>
 <div class="right">
            <img src="img/Agisoft_report.png" width ="90%">
 </div>
 <div class="left">   
<p> Includes: </p> 
    <ul>
    <li class="small fragment">Orthophoto and digital elevation model sketch;</li>
    <li class="small fragment">Camera parameters and survey scheme</li>
    <li class="small fragment">Tie points data export (matching points and panoramas) </li>
    <li class="small fragment">Image overlap statistics</li>
    <li class="small fragment">Camera positioning error estimates</li>
    <li class="small fragment">Ground control point error estimates</li>
    </ul> 
 </div>   
</section>
<section>
    <h2>Batch processing</h2>
             <img class="fragment" src="img/Agisoft_batch.png">
 </section>

 <section>
    <h3>Quality processing with GCPs</h3>
        <ul>
            <li class="fragment">Marker positions are defined by their projections on the source photos;</li>
            <li class="fragment">After optimizing alignment based on markers, Point cloud generation and other steps need to be performed</li>
            <p class="fragment">used for:</p>
            
            <ul>
                <li class="fragment">setting up a coordinate system,</li>
                <li class="fragment">photo alignment optimization,</li>
                <li class="fragment">measuring distances and volumes,</li>
                <li class="fragment">marker based chunk alignment.</li>
        </ul> 
            <li class="fragment">more on GCP placing and processing in Agisoft see see <a href="./2017_Imagery_Processing_assignment_intro.html">intro to the assignment</a></li>
</section>
 <section>
<h2> What did we learn?</h2>
<ul>
<li class="fragment">What is a general workflow for UAS imagery processing
<li class="fragment">How do we transform UAS data into orthophoto, DSM, 3D model and pointcloud
<li class="fragment">How to process the data in Agisoft Photoscan Professional and how to set proper parameters in the program
</ul>
</section>



